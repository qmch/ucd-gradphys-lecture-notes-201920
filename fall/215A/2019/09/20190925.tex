\subsection*{Motivation}
The aim of this course is to revisit quantum mechanics in more depth and more mathematical rigor. The official course text is Shankar's Principles of Quantum Mechanics, but the lecturer also recommends Weinberg's Quantum Mechanics, Sakurai, and Leslie Valentine. To really understand the foundations of quantum mechanics, Dirac's book is also useful.

\subsection*{Origins of quantum mechanics}
One of the original motivations for quantum mechanics was explaining the blackbody spectrum of thermal radiation. The Rayleigh-Jones curve described the small-temperature limit, and Planck later added a correction, exponential damping at high $T$. Underlying Planck's model was some suggestion of quantized, discrete energies, and this theory was further developed with Schr\"odinger's wave mechanics.

Today, there have been many precision tests of quantum mechanics confirming that quantum mechanics is a good model of small-scale phenomena. We'll spend the rest of this lecture discussing the mathematical formalism which makes quantum mechanics possible.

\subsection*{Mathematical background}
From a mathematical perspective, quantum mechanics is basically infinite-dimensional%
    \footnote{Well, finite in spin systems and so on.}
linear algebra. Why should we care about linear algebra? The key idea is this-- quantum mechanics obeys a superposition principle. Our theory is linear, so we can construct new solutions by taking linear combinations of existing ones.

To build this theory, we need objects living in a vector space and some operators acting on those vectors. More precisely, we will deal with 
\begin{itemize}
    \item wavefunctions, i.e. states in a Hilbert space
    \item \emph{calculables}, representing operator expectation values.%
        \footnote{We won't get into the details of measurement or observation in this course.}
\end{itemize}

\begin{defn}
    A \term{linear vector space} is a collection of elements called \term{vectors}, denoted by kets $\set{\ket{v_i}}$, on which two operations are defined: 
    \begin{itemize}
        \item Addition, $+: \ket{v},\ket{w}\to \ket{v}+\ket{w}$,
        \item and scalar multiplication, $\alpha \ket{v}, \alpha \in \CC.$
    \end{itemize} 
    These operations satisfy the following properties. For all $\ket{v_i},\ket{v_j}\in V$, $\alpha\in \CC$,
    \begin{itemize}
        \item $\ket{v_i}+\ket{v_j}\in V$
        \item $\alpha\ket{v_i} \in V$.
    \end{itemize}
\end{defn}

From these axioms, it follows that all linear combinations (superpositions) of vectors are allowed. That is, for $\ket{v_i},\ket{v_j}\in V, \alpha_i,\alpha_j\in \CC$,
\begin{equation}
    \alpha_i \ket{v_i} +\alpha_j \ket{v_j}\in V.
\end{equation}
%
Using the axioms, we can also prove some useful properties:
\begin{itemize}
    \item $\exists\, \ket{0}\in V$ (additive identity) such that $\ket{v}+\ket{0}=\ket{v}$.
    \item For all $\ket{v}\in V$, $\exists\, \ket{-v} \in V$ (additive inverse) such that $\ket{v}+\ket{-v}=\ket{0}$.
\end{itemize}
We will often be sloppy with our notation and denote $\ket{0}\sim 0$, so that $\ket{v}-\ket{v}=0$, which is secretly the zero vector.%
    \footnote{However, we will sometimes indicate a ground (vacuum) state by $\ket{0}$ instead. The harmonic oscillator (and all of quantum field theory) uses this notation. In that case, the ground state is usually defined to be the state that is annihilated to the true zero vector by the annihilation operators of the theory. That is, $a\ket{0}=0$.}

It would be frustrating if our set of vectors was simply impossible to manage, i.e. if the vectors had no nontrivial relationships between each other.%
    \footnote{Free fields in abstract algebra are kind of like this.}
Therefore, we will introduce the following definition.
\begin{defn}
    A set of vectors $\set{\ket{w_i}}$ comprises a \term{linearly independent set} if no nontrivial linear combination of them sums to zero, i.e. if
    \begin{equation}
        \sum_i \alpha_i \ket{w_i} =0 \implies \alpha_i = 0.
    \end{equation}
\end{defn}
\begin{defn}
    A set of vectors which is not linearly independent (there exists some combination $\alpha_i$ not all zero such that $\sum \alpha_i \ket{w_i}=0$) is called linearly dependent.
\end{defn}

Linear independence allows us to pick a special \emph{basis set}, which we denote $\set{\ket{e_n}}$. This set is chosen such that for \emph{any} $\ket{v}\in V$, there exists a decomposition
\begin{equation}
    \ket{v} =\sum \alpha_n \ket{e_n}.
\end{equation}
%This is just an abstraction of the familiar concept of the components of a vector; choosing a basis is equivalent to choosing a coordinate system.
Let's illustrate this with some examples.

\begin{exm}
    The space $\RR^3$ is a (real) vector space, where vectors can be denoted
    \begin{equation}
        \vec{v}= x \hat e_x + y \hat e_y + z \hat e_z.
    \end{equation}
    Equivalently in ket notation we could write
    \begin{equation}
        \ket{v}= x \ket{e_x} + y \ket{e_y} + z \ket{e_z}.
    \end{equation}
    This generalizes in the obvious way to $\RR^n$.
\end{exm}
\begin{exm}
    Consider a 1-qubit system, a quantum spin system with two states. A general state is written%
        \footnote{Some people prefer to denote the basis states as $\ket{\uparrow}\ket{\downarrow}$ or $\ket{+},\ket{-}$.}
    \begin{equation}
        \alpha\ket{0} +\beta\ket{1},
    \end{equation}
    with $\alpha,\beta\in \CC$.
    This is a complex vector space, because the coefficients $\alpha$ and $\beta$ are complex.
\end{exm}
\begin{exm}
    We can define a \term{discretuum vector space} with a basis set $\set{\ket{n}}_{n=0,1,2,\dots}$. A general element can be written as
    \begin{equation}
        \sum_{n=0}^\infty \alpha_n \ket{n},
    \end{equation}
    with $\alpha_n\in \CC$. This is of course the space of states for the harmonic oscillator, or more generally any confining potential.
\end{exm}
\begin{exm}
    Consider the space spanned by $2\times 2$ matrices with complex entries. In particular, the following set of four matrices forms a basis for this space:%
        \footnote{These matrices are linearly independent, as one can check by taking a general linear combination. Moreover, the dimension is correct. We need four complex parameters to specify a general element in this space, so it has complex dimension 4. But we've constructed a set of four linearly independent elements, so a linear combination of them will have four complex parameters, which is exactly right to form a basis.}
    \begin{equation}
        \sigma_1 =\begin{pmatrix}
            0 & 1\\
            1 & 0
        \end{pmatrix},\,
        \sigma_2= \begin{pmatrix}
            0 & -i\\
            i & 0
        \end{pmatrix},\,
        \sigma_3= \begin{pmatrix}
            1 & 0\\
            0 & -1
        \end{pmatrix},\,
        \II_2=\begin{pmatrix}
            1 & 0\\
            0 & 1
        \end{pmatrix}.
    \end{equation}
\end{exm}
\begin{exm}
    We could have a continuum vector space which is the space of functions over some domain.%
        \footnote{Strictly this is a countable vector space with continuum elements. For a vector space with uncountably infinite basis elements, consider plane waves and a Fourier decomposition.}
    In particular, consider the set of Hermite polynomials $\ket{n}=H_n(x)$, $x\in \RR$, the solutions to the harmonic oscillator potential. This is just a function representation of the solutions. Similarly, the Bessel functions from electromagnetism and spherical harmonics from solving the Laplacian in spherical coordinates also form vector spaces.
\end{exm}

The rest of today's discussion centers on how to choose a useful basis.

\begin{defn}
    If a vector can be written as
    \begin{equation}
        \ket{v}=\sum_n \alpha_n \ket{e_n}
    \end{equation}
    with respect to a basis $\set{\ket{e_n}}$, we say that the coefficients $\alpha_n$ are the \emph{components} of $\ket{v}$ in the basis $\set{\ket{e_n}}$.
\end{defn}
In a different basis, the components will generally change, but the \emph{vector does not}.%
    \footnote{This is a critical fact in general relativity, where vectors are defined in a tangent bundle.}

\subsection*{Normed vector space}
It's possible to do linear algebra without an inner product. But we're physicists, so we shall define one.
\begin{defn}
    An \term{inner product} is a function of two vectors, denoted in bra-ket notation as 
    \begin{equation}
        \paren{\ket{v},\ket{w}}=\braket{v}{w},
    \end{equation}
    and which obeys the following properties:
    \begin{itemize}
        \item $\braket{v}{w}\in \CC$
        \item $\braket{v}{w}=\braket{w}{v}^*$
        \item $\braket{v}{\alpha_1 w_1 +\alpha_2 w_2} = \alpha\braket{v}{w_1} +\alpha_2 \braket{v}{w_2}$
        \item $\braket{v}{v}\geq 0$ with $\braket{v}{v}=0\iff \ket{v}=0$.
    \end{itemize}
\end{defn}
Inner products generalize the notion of a (scalar) dot product, which measures how parallel two vectors are to one another.
\begin{defn}
    We can then define the \emph{norm} of a vector as
    \begin{equation}
        ||v||=\sqrt{\braket{v}{v}}.
    \end{equation}
\end{defn}
\begin{ex}
    Show that the inner product is antilinear in the first argument, i.e.
    \begin{equation}
        \braket{\alpha_1 v_1 +\alpha_2 v_2}{w} = \alpha_1^* \braket{v_1}{w} + \alpha_2^* \braket{v_2}{w}.
    \end{equation}
\end{ex}

We can derive some useful properties of an inner product. The inner product must satisfy the \term{Schwarz inequality},
\begin{equation}
    |\braket{w}{v}|^2 \leq \braket{w}{w} \braket{v}{v}.
\end{equation}
Recall that in $\RR^3$, this is just the statement that $\braket{w}{v}=\vec{v} \cdot \vec{w} = |\vec{v}||\vec{w}|\cos\theta.$ We can prove it by considering the vector
\begin{equation}
    \ket{z} = \ket{v} - \frac{\braket{w}{v}}{||w||^2}\ket{w}
\end{equation}
and using the positivity of the norm of $\ket{z}$.

The inner product also satisfies the \term{triangle inequality},
\begin{equation}
    ||v+w||\leq ||v|| + ||w||.
\end{equation}

A general basis $\set{\ket{e_n}}$ can be made orthonormal, i.e. chosen/defined so that the elements obey
\begin{equation}
    \braket{e_n}{e_m} = \delta_{nm}.
\end{equation}
This can always be done by the Gram-Schmidt algorithm.%
    \footnote{See my 204A notes for the details.}

In an orthonormal basis, the inner product between two vectors is very easy to calculate. With $\ket{v}=\sum \alpha_n \ket{e_n}, \ket{w} =\sum \beta_n \ket{e_n},$ the inner product is given by
\begin{equation}
    \braket{v}{w}=\sum_n \alpha_n^* \beta_n.
\end{equation}

\subsection*{Dual vector space}
We wish to use the inner product to define a \term{dual vector space}. Strictly we don't need an inner product to do this, but it provides a natural way to do so.%
    \footnote{This generalizes beautifully to curved spacetimes in general relativity. An inner product (metric) $g_{ab}$ defines a map from two vectors to a scalar, $g_{ab}:V\times V \to \CC$, given by $g_{ab}V^a W^b$. But it also naturally defines a map from vectors $V^a$ to dual vectors $V_b = g_{ab} V^a$, where the dual vector $V_b$ is now a map from a single vector to a scalar, $V_b: V \to \CC$ defined by $V_b W^b$. The only complication is that a metric need not be positive semi-definite, so norms can be negative when we measure distances in spacetime. You might know this as the difference between timelike and spacelike separations.}
\begin{defn}
    Given a vector space $V$, we define the dual vector space $V^*$, whose elements are \emph{linear functionals} on $V$. That is, if $F_w \in V^*$, then
    \begin{equation}
        F_w:V\to \CC
    \end{equation}
    such that
    \begin{equation}
        F_w(\alpha_1 \ket{v_1} + \alpha_2 \ket{v_2}=\alpha_1 F_w (\ket{v_1}) +\alpha_2 F_ (\ket{v_2}).
    \end{equation}
\end{defn}

Moreover, we claim that the dual vector space deserves its name; it is an honest vector space. This is pretty easy to check, i.e. that linear combinations of linear functionals are themselves linear functionals.

For a ket vector $\ket{v}\in V$, we can associate a bra vector $\bra{v}\in V^*$. Given a basis $\set{\ket{e_n}}$ for $V$, we may define a dual basis $\set{\bra{e_m}}$ for $V^*$, defined such that
\begin{equation}
    \braket{e_m}{e_n}=\delta_{mn}.
\end{equation}
Note that this immediately implies that any ket vector has a corresponding bra vector in the natural way. That is, we may define an \term{adjoint operation} such that for
\begin{equation}
    \ket{v}=\sum_n \alpha_n \ket{e_n},
\end{equation}
there exists a corresponding bra vector
\begin{equation}
    \bra{v}=\sum_n \alpha_n^* \bra{e_n}.
\end{equation}
In this sense, the vector space $V$ and its dual vector space $V^*$ are isomorphic.

We can also write the vectors in a funny way:
\begin{equation}
    \ket{v}=\sum_n \braket{e_n}{v}\ket{e_n}
\end{equation}
and similarly
\begin{equation}
    \bra{v} =\sum_m \braket{v}{e_m} \bra{e_m}.
\end{equation}
That is, the components of a vector are simply given by its projections onto the basis vectors.